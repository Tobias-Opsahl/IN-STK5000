{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project for Project 2 in IN-STK5000\n",
    "## Alva HÃ¸rlyk, Tobias Opsahl and Ece Cetinoglu\n",
    "\n",
    "This notebook is contains what will be the final version of Project 2. Since we have changed a bit under the way, the parts for the 1st and 2nd deadline is a bit different, and we therefore deliver the whole project in this notebook. We go through the exercises starting on the 1st deadline, and ending on the last one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information\n",
    "\n",
    "First of all, let us introduce our policy. We will look at treaments, and not vaccines.\n",
    "\n",
    "## Utility\n",
    "To calculate the utility, we look at each person, their symptoms, treaments and outcomes. For each person we calculate the reward, and the utility is simply the sum of the rewards. The reward is calculated by looking at each symptom. \n",
    "If the symptom was there prior to the treatment given, and not afterwards, a positive weight is added. If the symptom was not there before the treatment, but was there afterwards, a negative weight is added, times a penalty. Finally, if any treatment was used (it is not the case that no treatments where used), a treatment-cost is subtracted from the reward.\n",
    "\n",
    "## Policy overview\n",
    "\n",
    "We will provide a short overview of the policy here. The actual code should be pretty well documented, so you can find more details in the doc-strings. \n",
    "\n",
    "The policy takes the amount of treatments as an argument in the **constructor**, which we assume to be 3 for compatibility with the **simulator.py**. The method **observe()** takes in a poluation (features), actions and outcomes, and fit the models used to predict the outcomes. Note that one would need the population, the actions and the outcomes from treating the poluation with the actions (by using **population.treat()**), before calling the method, since they are required as arguments. To chose the actions for the initial call, one can use **initialize_data()**, to pick actions randomly, and getting the corresponding outcomes. For each treatment, for each symptom, we fit a logistic regression model. Given a treatment and a symtpom, the model takes in some columns from the features as input, and returns the probability for that symptom to be present after the given treatment. \n",
    "The method **feature_select()** will chose the columns we want to fit the model on. This is the given symptom, age, income, gender, and the comorbidities. We do not select the other symptoms, because we assume they are not realated. We do not use the genes, becuase after our analysis they seem to be unrealated to the response, and adds a lot of noise.\n",
    "**get_reward()** calculates the individual reward for each person in features, and **get_utility()** is the sum of the rewards. This was explained in the introductions and there are more details in the code. \n",
    "**get_action()** will chose a suitable treatment for each person in features. Note that **observe()** should have been called first. We use the fitted models to predict the probability of symptoms after a treatment, for each treatment. We then use the probability for symptoms as a response to **get_reward()**, to calculate the expected reward. For each person, we chose the treatment with the highest expected reward. If all of the treatments gives negative expected rewards, we chose no treatment. \n",
    "\n",
    "The help functions **add_feature_names()**, **add_action_names()** and **add_outcome_names()** sumply converts features, actions and outcomes to a pandas dataframe, with suitable column-names. \n",
    "\n",
    "**treatments_given()** checks how many persons recieved at least one treatment. \n",
    "\n",
    "We also make a class **ZeroModel**, to only predict zero if the response is only zero. This is because **LogisticRegression** does not work when all of the responses are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the imports. Note that we renamed aux.py to aux_file.py to work on windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aux_file import symptom_names\n",
    "import simulator\n",
    "from IPython import embed\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the actual code for the policy. We also include the RandomPolicy, since it is used in the initialization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, n_actions, action_set, threshold=0.5):\n",
    "        \"\"\" \n",
    "        In:\n",
    "            n_actions (int): the number of actions\n",
    "            action_set (list): the set of actions\n",
    "            threshold (float): Threshold for caterogizing from logistic regression\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.action_set = action_set\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def initialize_data(self, n_population):\n",
    "        \"\"\"\n",
    "        This function should be called before using the rest of the methods.\n",
    "        This function simply makes a RandomPolicy of the same length as the\n",
    "        population will be. This data is then used to fit the models later. \n",
    "        In: \n",
    "            n_population (int): Size of population, the amount of persons.\n",
    "        Out:\n",
    "            features (np.array): The population, generated by simulator.py.\n",
    "            actions (np.array): The actions chosen by RandomPolicy on features.\n",
    "            outcomes (np.array): The outcomes when features are treated with actions.\n",
    "        These are also stored as class variables. \n",
    "        \"\"\"\n",
    "        population = simulator.Population(128, 3, 3)\n",
    "        treatment_policy = RandomPolicy(3, list(range(3))) \n",
    "        self.n_population = n_population\n",
    "        self.features = population.generate(self.n_population)\n",
    "        self.actions = treatment_policy.get_action(self.features)\n",
    "        self.outcomes = population.treat(list(range(n_population)), self.actions)\n",
    "        return self.features, self.actions, self.outcomes\n",
    "    \n",
    "    def feature_select(self, X, symptom_index=1):\n",
    "        \"\"\"\n",
    "        Chooses some columns in X.\n",
    "        0 Covid-Recovered\n",
    "        1 Covid-Positive\n",
    "        2 No-Taste/Smell\n",
    "        3 Fever\n",
    "        4 Headache\n",
    "        5 Pneumonia\n",
    "        6 Stomach\n",
    "        7 Myocarditis\n",
    "        8 Blood-Clots\n",
    "        9 Death\n",
    "        10 Age\n",
    "        11 Gender\n",
    "        12 Income\n",
    "        141 Asthma\n",
    "        142 Obesity\n",
    "        143 Smoking\n",
    "        144 Diabetes\n",
    "        145 Heart disease\n",
    "        146 Hypertension\n",
    "        \"\"\"\n",
    "        N = X[:, [symptom_index, 10, 11, 12, 141, 142, 143, 144, 143, 144, 145, 146]]\n",
    "        return N\n",
    "        \n",
    "    def get_reward(self, features, actions, outcome, penalty=1.5, treatment_cost=0.1):\n",
    "        \"\"\"\n",
    "        This method calculates the reward, which in a way is the utility of \n",
    "        a single indidual, for each row in the arguments.\n",
    "        We calculate this by the following:\n",
    "            For each person, if that person experience some symptom before the,\n",
    "        treatment, but not after, a positive weight is added to the reward. If\n",
    "        they did not experience any symptoms before, but do after the action, \n",
    "        then a negative weight is added, times a penalty (so new sick \n",
    "        individuals can be punished harder than curing sick persons is rewarded).\n",
    "        The weight corresponds to the severity of the symptoms. \n",
    "        Finally, if we apply any action at all, a treatment_cost is subtracted, \n",
    "        representing the cost of a treatment. \n",
    "        \n",
    "        In:\n",
    "            features (np.array): The population.\n",
    "            actions (np.array): The actions.\n",
    "            outcome (np.array): The outcome when population is treated with outcome.\n",
    "            penalty (float): How much worse it is to give a new symptom, than \n",
    "                to cure one. If 0, the reward overlooks if a person gets a new\n",
    "                symptom. When penalty -> \\infty, the reward only prevents new\n",
    "                symptoms, and do not care about the cured patients.\n",
    "            treatment_cost (float): The cost of a treatment. If one of the \n",
    "                columns in actions is non-zero, treatment_cost is subtracted\n",
    "                from the reward. If not, then no treatment is used, and \n",
    "                we do not subtract it. \n",
    "        Out:\n",
    "            rewards (np.array): Array of rewards, corresponding to the persons\n",
    "                in features (and actions and outcome).\n",
    "                \n",
    "        \"\"\"\n",
    "        rewards = np.zeros(len(outcome))\n",
    "        weights = [0, 0.2, 0.1, 0.1, 0.1, 0.5, 0.2, 0.5, 1.0, 100.0]\n",
    "        threshold = self.threshold\n",
    "        for t in range(len(features)):\n",
    "            utility = 0\n",
    "            for i in range(1, len(weights)): # i loops over the sypmtom indecies\n",
    "                if features[t, i] == 1 and outcome[t, i-1] < threshold:\n",
    "                    utility += weights[i]\n",
    "                if features[t, i] == 0 and outcome[t, i-1] >= threshold:\n",
    "                    utility -= weights[i] * penalty\n",
    "            if (np.sum(actions[t, :]) > 0): # Some action were used\n",
    "                utility = utility - treatment_cost # The treatment is not free\n",
    "            rewards[t] = utility \n",
    "        return rewards\n",
    "        \n",
    "    def observe(self, features, actions, outcomes):\n",
    "        \"\"\"\n",
    "        This functions takes in a population, the action used on them, and the\n",
    "        outcome from doing so. Then it will update the model. \n",
    "        The model is updated accordingly:\n",
    "            For each treatment (which should be 3), for each symptoms (which is\n",
    "        9, Covid-Recovered is overlooked), a logistic regression method is\n",
    "        fitted. The models are stored as class variable. If all of the responses\n",
    "        are 0, then a model constantly predicting 0 is used. The models are\n",
    "        fitted for each action for each symptoms with the features and action\n",
    "        as input, and the post_symptom (symptom in outcomes) as response. \n",
    "        IN OTHER WORDS: Each model predicts wether a certain treatment will \n",
    "        for a certain symptom will continue to be there, after the treatment.\n",
    "            NOTE: To start the method, one could get the inputs by calling\n",
    "        initialize_data(), to get a random starting point to fit the data on.\n",
    "        \n",
    "        In: \n",
    "            features (t*|X| array): The population\n",
    "            actions (t*|A| array): The actions the population is treated with\n",
    "            outcomes (t*|Y| array): The outcomes from treating the population\n",
    "                with the actions.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.actions = actions\n",
    "        self.outcomes = outcomes\n",
    "        symptom_indecies = [1, 2, 3, 4, 5, 6, 7, 8, 9] # Indecies for symptoms\n",
    "        models = []\n",
    "        for treatment in range(self.n_actions): # for each treatment i\n",
    "            indecies = self.actions[:, treatment] == 1 # treament i is used\n",
    "            for symptom_index in symptom_indecies: # for each symptom\n",
    "                feat = self.features[indecies]\n",
    "                out = self.outcomes[indecies]\n",
    "                x_data = self.feature_select(feat, symptom_index)\n",
    "                y_data = out[:, symptom_index]\n",
    "                logistic_model = LogisticRegression()\n",
    "                scaler = preprocessing.StandardScaler().fit(x_data)\n",
    "                x_scaled = scaler.transform(x_data)\n",
    "                if sum(y_data) != 0: \n",
    "                    logistic_model.fit(x_scaled, y_data)\n",
    "                    # model = logistic_model.fit(x_scaled, y_data)\n",
    "                else: # If all y_data is 0, we just predict 0. LogisticRegression would crash\n",
    "                    logistic_model = ZeroModel(\"Name\")\n",
    "                models.append(logistic_model)\n",
    "        self.models1 = models[:9] # Treatments / n_actions should be 3\n",
    "        self.models2 = models[9:18]\n",
    "        self.models3 = models[18:]\n",
    "        \n",
    "    def get_utility(self, features, actions, outcome, penalty=1.5, treatment_cost=0.1):\n",
    "        \"\"\" \n",
    "        Return the empirical utility for a population. This is defined by the\n",
    "        sum of the rewards, so se Policy.get_reward() for explaination of how\n",
    "        we define our utility.\n",
    "        \n",
    "        Args:\n",
    "            features (t*|X| array)\n",
    "            actions (t*|A| array)\n",
    "            outcomes (t*|Y| array)\n",
    "            penalty (float): penalty for introducing new symptoms.\n",
    "            treatment_cost (float): Cost for using a treatment\n",
    "        Out:\n",
    "            utility (float): Empirical utility of the policy on this data.\n",
    "        \"\"\"\n",
    "        utility = sum(self.get_reward(features, actions, outcome, penalty, treatment_cost))\n",
    "        return utility\n",
    "        \n",
    "    def get_action(self, features):\n",
    "        \"\"\"\n",
    "        Get actions for one or more people. observe() should already have been\n",
    "        called, so the model is fitted. \n",
    "        The actions are chosen as follows:\n",
    "            For each person in the dataset, we use the models to extimate the \n",
    "        probability for a treatment introducing or removing a symptom, for\n",
    "        each symptom. Then we have a the probability for the post_symptoms, \n",
    "        which behaves as the \"expected response\". For each person, we then\n",
    "        see which treatment gives the largest expected reward. If all the \n",
    "        expected rewards is below zero, we do not treat them, if at least one\n",
    "        is positive, we chose the one that is largest.\n",
    "        In: \n",
    "            features (t*|X| array): Population to be found an action on\n",
    "        Out: \n",
    "            actions (t*|A| array): Actions chosen\n",
    "        \"\"\"\n",
    "        symptom_indecies = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        post_symptoms1 = np.zeros((len(features), len(symptom_indecies)))\n",
    "        post_symptoms2 = np.zeros((len(features), len(symptom_indecies)))\n",
    "        post_symptoms3 = np.zeros((len(features), len(symptom_indecies)))\n",
    "        \n",
    "        for symptom_index in symptom_indecies: \n",
    "            x_data = self.feature_select(features, symptom_index)\n",
    "            scaler = preprocessing.StandardScaler().fit(x_data)\n",
    "            x_scaled = scaler.transform(x_data)\n",
    "            pred1 = self.models1[symptom_index - 1].predict_proba(x_scaled)[:, 1]\n",
    "            pred2 = self.models2[symptom_index - 1].predict_proba(x_scaled)[:, 1]\n",
    "            pred3 = self.models3[symptom_index - 1].predict_proba(x_scaled)[:, 1]\n",
    "            post_symptoms1[:, symptom_index-1] = pred1\n",
    "            post_symptoms2[:, symptom_index-1] = pred2\n",
    "            post_symptoms3[:, symptom_index-1] = pred3\n",
    "        \n",
    "        mock_actions = np.ones((self.n_population, 3)) # Represent an actions has been done\n",
    "        rewards1 = self.get_reward(features, mock_actions, post_symptoms1)\n",
    "        rewards2 = self.get_reward(features, mock_actions, post_symptoms2)\n",
    "        rewards3 = self.get_reward(features, mock_actions, post_symptoms3)\n",
    "        \n",
    "        actions = np.zeros([n_population, self.n_actions]) # Initialize\n",
    "        for t in range(n_population):\n",
    "            # print(f\"1: {pred1[t]} 2: {pred2[t]} 3: {pred3[t]}\")\n",
    "            if np.max(np.asarray([rewards1[t], rewards2[t], rewards3[t]])) < 0:\n",
    "                # All the treatments have expected utility less than zero\n",
    "                actions[t, 0] = 0 # Do nothing\n",
    "            # If at least one expected reward is bigger than 0, we chose the biggest\n",
    "            elif rewards1[t] >= rewards2[t] and rewards1[t] >= rewards3[t]:\n",
    "                actions[t, 0] = 1\n",
    "            elif rewards2[t] >= rewards1[t] and rewards2[t] >= rewards3[t]:\n",
    "                actions[t, 1] = 1\n",
    "            elif rewards3[t] >= rewards1[t] and rewards3[t] >= rewards2[t]:\n",
    "                actions[t, 2] = 1\n",
    "        # embed()\n",
    "        return actions\n",
    "    \n",
    "    def get_arguments(self):\n",
    "        return self.features, self.actions, self.outcomes\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\" This is a purely random policy!\"\"\"\n",
    "\n",
    "    def get_utility(self, features, action, outcome):\n",
    "        \"\"\"Here the utiliy is defined in terms of the outcomes obtained only, ignoring both the treatment and the previous condition.\n",
    "        \"\"\"\n",
    "        actions = self.get_action(features)\n",
    "        utility = 0\n",
    "        utility -= 0.2 * sum(outcome[:,symptom_names['Covid-Positive']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Taste']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Fever']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Headache']])\n",
    "        utility -= 0.5 * sum(outcome[:,symptom_names['Pneumonia']])\n",
    "        utility -= 0.2 * sum(outcome[:,symptom_names['Stomach']])\n",
    "        utility -= 0.5 * sum(outcome[:,symptom_names['Myocarditis']])\n",
    "        utility -= 1.0 * sum(outcome[:,symptom_names['Blood-Clots']])\n",
    "        utility -= 100.0 * sum(outcome[:,symptom_names['Death']])\n",
    "        return utility\n",
    "    \n",
    "    def get_action(self, features):\n",
    "        \"\"\"Get a completely random set of actions, but only one for each individual.\n",
    "        If there is more than one individual, feature has dimensions t*x matrix, otherwise it is an x-size array.\n",
    "        \n",
    "        It assumes a finite set of actions.\n",
    "        Returns:\n",
    "        A t*|A| array of actions\n",
    "        \"\"\"\n",
    "\n",
    "        n_people = features.shape[0]\n",
    "        ##print(\"Acting for \", n_people, \"people\");\n",
    "        actions = np.zeros([n_people, self.n_actions])\n",
    "        for t in range(features.shape[0]):\n",
    "            action = np.random.choice(self.action_set)\n",
    "            if (action >= 0):\n",
    "                actions[t,action] = 1\n",
    "            \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the help functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_names(X):\n",
    "    \"\"\"\n",
    "    Convert a population / features / X to a pandas dataframe with suitable names.\n",
    "    \"\"\"\n",
    "    features_data = pd.DataFrame(X)\n",
    "    features = []\n",
    "    features += [\"Covid-Recovered\", \"Covid-Positive\", \"No-Taste/Smell\", \"Fever\", \n",
    "                 \"Headache\", \"Pneumonia\", \"Stomach\", \"Myocarditis\", \n",
    "                 \"Blood-Clots\", \"Death\"]\n",
    "    features += [\"Age\", \"Gender\", \"Income\"]\n",
    "    features += [\"Genome\" + str(i) for i in range(1, 129)]\n",
    "    features += [\"Asthma\", \"Obesity\", \"Smoking\", \"Diabetes\", \n",
    "                 \"Heart disease\", \"Hypertension\"]\n",
    "    features += [\"Vaccination status\" + str(i) for i in range(1, 4)]\n",
    "    features_data.columns = features\n",
    "    return features_data\n",
    "    \n",
    "def add_action_names(actions):\n",
    "    \"\"\"\n",
    "    Convert np.array of actions to a pandas dataframe with suitable names.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(actions)\n",
    "    names = [\"Treatment\" + str(i) for i in range(1, actions.shape[1] + 1)]\n",
    "    df.columns = names\n",
    "    return df\n",
    "\n",
    "def add_outcome_names(outcomes):\n",
    "    \"\"\"\n",
    "    Convert a np.array of outcomes / post_symptoms to a pandas dataframe with\n",
    "    suitable names. \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(outcomes)\n",
    "    columns = [\"Covid-Recovered\", \"Covid-Positive\", \"No-Taste/Smell\", \"Fever\", \n",
    "                  \"Headache\", \"Pneumonia\", \"Stomach\", \"Myocarditis\", \n",
    "                  \"Blood-Clots\", \"Death\"]\n",
    "    for i in range(len(columns)):\n",
    "        columns[i] = \"Post_\" + columns[i]\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "def treatments_given(actions):\n",
    "    \"\"\"\n",
    "    Given a set of actions, this function will return how many the patient who\n",
    "    recieved at least one action, in other words, do not have 0 for every\n",
    "    action column.\n",
    "    \"\"\"\n",
    "    actions = np.asmatrix(actions)\n",
    "    s = 0\n",
    "    for i in range(len(actions)):\n",
    "        if np.sum(actions[i, :]) > 0:\n",
    "            s += 1\n",
    "    return s\n",
    "\n",
    "class ZeroModel:\n",
    "    \"\"\"\n",
    "    This class is simply made for always prediction 0. LogisticRegression\n",
    "    does not work for only 0 inputs in the response, so we do this instead.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"name\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def predict(self, array):\n",
    "        \"\"\"\n",
    "        0 or 1 predictions, which should always be 0.\n",
    "        \"\"\"\n",
    "        return np.zeros(len(array))\n",
    "    \n",
    "    def predict_proba(self, array):\n",
    "        \"\"\"\n",
    "        Probability predictions for \"yes\" and \"no\" for each input, 0 and 1.\n",
    "        \"\"\"\n",
    "        prob = np.zeros((len(array), 2))\n",
    "        prob[:, 1] = np.ones(len(array))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Deadline\n",
    "\n",
    "We will now explore ways to add privacy to the data and the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "One simple way to protect the private information of the individuals is to just hide direct identifiers. However this is generally insufficient as attackers may have other identifying information. This information, combined with the information in the database, can reveal identities. \n",
    "<br>\n",
    "Another method is k-anonymization, where k-1 people are indistinguishable from each other (with respect to quasi-identifiers) in the database. Columns with personal information, like name and date of birth are removed, and the rest of the information is generalized. For instance can a variable like age be categorical with different age-groups. Even though k-anonymization is an improvement from simply removing direct identifiers, an attacker with enough imformation can still infer something about the individuals.\n",
    "<br>\n",
    "If we assume that an attacker can have a lot of side-information, it is better to use differential privacy. For instance, we can use the Laplace mechanism, where Laplace distributed noise is added in the model. How much noise we add determines how private the result is. We can randomly chose a fraction of the data and add noise to it. This way, even if the data was publicly available, one would not be certain if the it really was true. The goal would obviously add a fraction of noise that makes the data private enough, but do not lower the predictions significantly. \n",
    "<br>\n",
    "In this task the policy is released and can be used by the public. Then the data have to be anonymized before $\\pi(a|x)$ is obtained. This can be done using a local privacy model, where independent Laplace noise $\\omega_{i}$ is added to each individual. We have $y_{i}=x_{i}+\\omega_{i}$ and use it to get $a=n^{-1}\\sum_{i=1}^{n}y_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) \n",
    "Here we assume that the analysts can be trusted with private information, so only the result made available for the public have to be privatized. Then we can use a centralized privacy model. We obtain $\\pi(a|x)$ with $a=n^{-1}\\sum_{i=1}^{n}x_{i}+\\omega$. We do not need to privatize the data, just a bit of the decisions of the model we fitted on it. \n",
    "\n",
    "In other words, we can add noise to the actions after fitting the model. Without changing any of the observations in the population, we can fit a model that decides actions, and then add a bit of noise to the actions. This is so it one could not figure out personal data based on the action we chose, which might happen if the model picks ups a simple pattern in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual implementation, we use both approaches. First, we add noise to the data itself and fit a model. Then we try to fit a model first, then add noise to the results. \n",
    "For the first approach, we implement functions for adding noise to our data. For the binary data, the function randomize() choses a ratio \"1-theta\" from a column, and changes it with a coinflip (50-50 chance of 0 and 1). For the continious variables, we replace the coinflip with the same data pluss an addition drawn from the Laplace distribution.\n",
    "\n",
    "We then loop over all the desired columns in our population and add noise one by one. The desired columns are Age, Income, Gender and the co-morbidities. We do not want to change the pre- or post-sympoms, because this will drasticly change the utility. For example, let's say that our noise happend to remove a lot of the positive post-symptoms. Then this would artificially make our model look way better than it actually would be. The utility is calculated directly on the pre-symptom, action and post-symptom, so we do not add noise here. The model is then fitted on the privatized data. \n",
    "\n",
    "For the second approach, we simply fit the model on the data, then sends the outcome columns to the functions to add noise. We will do this two ways. First, we simply shuffle a persons actions with a \"1-theta\" probability. This may change the treatment a treated person is given, but it will not change a non-treated person, since all of their action-columns will correspond to 0. Therefore, we secondly draw 1, 2, 3 or no-treatment, with a 1-theta probability, for each person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) \n",
    "\n",
    "Let us now try to implement a policy, and see how the utility is affected by the privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def privatize_actions_shuffle(A, theta):\n",
    "    \"\"\"\n",
    "    Adds noise to the actions chosen by the model. This is done by shuffling \n",
    "    the actions given to each person, with a probability 1 - theta. \n",
    "    \"\"\"\n",
    "    A1 = A.copy()\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=A.shape[0])\n",
    "    for i in range(A1.shape[0]):\n",
    "        if not coins[i]:\n",
    "            np.random.shuffle(A1[i, :])\n",
    "    return A1\n",
    "\n",
    "def privatize_actions_draw(A, theta):\n",
    "    \"\"\"\n",
    "    Adds noise to the actions chosen by the model. This is done by choicing a \n",
    "    new action with a probability 1-theta.\n",
    "    \"\"\"\n",
    "    A1 = A.copy()\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=A.shape[0])\n",
    "    for i in range(A1.shape[0]):\n",
    "        if not coins[i]:\n",
    "            A1[i, :] = np.zeros(A.shape[1])\n",
    "            coin = np.random.randint(A.shape[1]+1)\n",
    "            if coin != A.shape[1]:\n",
    "                A1[i, coin] = 1\n",
    "    return A1\n",
    "    \n",
    "def randomize(a, theta):\n",
    "    \"\"\"\n",
    "    Randomize a single column. Simply add a coin-toss to 1- theta amount of the data\n",
    "    \"\"\"\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=a.size)\n",
    "    noise = np.random.choice([0, 1], size=a.size)\n",
    "    response = np.array(a)\n",
    "    response[~coins] = noise[~coins]\n",
    "    return response \n",
    "    \n",
    "def randomize_cont(a, theta, decay=1):\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=a.size)\n",
    "    noise = np.random.laplace(0, decay, a.size)\n",
    "    response = np.array(a)\n",
    "    response[~coins] = response[~coins] + noise[~coins]\n",
    "    return response\n",
    "    \n",
    "def privatize(X, theta):\n",
    "    \"\"\"\n",
    "    Adds noice to the data, column by column. The continious and discreet \n",
    "    columns are treated differently. \n",
    "    \"\"\"\n",
    "    df = add_feature_names(X).copy()\n",
    "    df[\"Age\"] = randomize_cont(df[\"Age\"], theta)\n",
    "    df[\"Income\"] = randomize_cont(df[\"Income\"], theta)\n",
    "    columns = [\"Gender\", \"Asthma\", \"Obesity\", \"Smoking\", \n",
    "               \"Diabetes\", \"Heart disease\", \"Hypertension\"]\n",
    "    for column in columns:\n",
    "        df[column] = randomize(df[column], theta)\n",
    "    symptoms = [\"Covid-Recovered\", \"Covid-Positive\", \"No-Taste/Smell\", \"Fever\", \n",
    "                \"Headache\", \"Pneumonia\", \"Stomach\", \"Myocarditis\", \"Blood-Clots\", \"Death\"]\n",
    "    for columns in symptoms: # Shuffle symptoms\n",
    "        df[column] = randomize(df[column], theta)\n",
    "    return np.asarray(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Now lets test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(57)\n",
    "n_genes = 128\n",
    "n_vaccines = 3\n",
    "n_treatments = 3\n",
    "n_population = 10000\n",
    "population = simulator.Population(n_genes, n_vaccines, n_treatments)\n",
    "np.random.seed(57)\n",
    "X = population.generate(n_population) # Population\n",
    "treatment_policy = Policy(n_treatments, list(range(n_treatments)))\n",
    "np.random.seed(57)\n",
    "features, actions, outcomes = treatment_policy.initialize_data(n_population)\n",
    "treatment_policy.observe(features, actions, outcomes)\n",
    "A = treatment_policy.get_action(X) # Actions \n",
    "U = population.treat(list(range(n_population)), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = [1, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0]\n",
    "utility_list1 = np.zeros(len(thetas) + 1)\n",
    "utility_list2 = np.zeros(len(thetas) + 1)\n",
    "utility_list3 = np.zeros(len(thetas) + 1)\n",
    "utility_list1[0] = treatment_policy.get_utility(X, A, U)\n",
    "utility_list2[0] = treatment_policy.get_utility(X, A, U)\n",
    "utility_list3[0] = treatment_policy.get_utility(X, A, U)\n",
    "for i in range(len(thetas)):\n",
    "    np.random.seed(57)\n",
    "    X_noise = privatize(X, thetas[i])\n",
    "    A_noise1 = treatment_policy.get_action(X_noise)\n",
    "    A_noise2 = privatize_actions_shuffle(A, thetas[i])\n",
    "    A_noise3 = privatize_actions_draw(A, thetas[i])\n",
    "\n",
    "    U1 = population.treat(list(range(n_population)), A_noise1)\n",
    "    U2 = population.treat(list(range(n_population)), A_noise2)\n",
    "    U3 = population.treat(list(range(n_population)), A_noise3)\n",
    "\n",
    "    utility_list1[i+1] = treatment_policy.get_utility(X, A_noise1, U1)\n",
    "    utility_list2[i+1] = treatment_policy.get_utility(X, A_noise2, U2)\n",
    "    utility_list3[i+1] = treatment_policy.get_utility(X, A_noise3, U3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2311.7 , 2208.  , 2813.15, 1600.45, 2493.5 , 1699.65, 1997.5 ,\n",
       "       1742.2 , 2550.55, 1946.  , 1848.7 , 1964.15, 2000.75, 2014.8 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2311.7 ,  803.8 , 2399.75, 2400.05, 2667.4 , 2086.6 , 1676.65,\n",
       "       1918.4 ,  978.3 ,  130.75, -384.2 ,  329.9 , -507.65,  -85.8 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2311.7 ,  2208.85,  2484.55,  2210.3 ,  2238.1 ,   538.85,\n",
       "         859.1 ,   -52.05,   411.1 , -2216.  , -2847.  , -2859.8 ,\n",
       "       -2346.85, -3889.5 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
