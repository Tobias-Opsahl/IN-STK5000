{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 IN-STK5000, First deadline\n",
    "## Group 10, Tobias Opsahl, Alva HÃ¸rlyk, Ece Centinogly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "One simple way to protect the private information of the individuals is to just hide direct identifiers. However this is generally insufficient as attackers may have other identifying information. This information, combined with the information in the database, can reveal identities. \n",
    "<br>\n",
    "Another method is k-anonymization, where k-1 people are indistinguishable from each other (with respect to quasi-identifiers) in the database. Columns with personal information, like name and date of birth are removed, and the rest of the information is generalized. For instance can a variable like age be categorical with different age-groups. Even though k-anonymization is an improvement from simply removing direct identifiers, an attacker with enough imformation can still infer something about the individuals.\n",
    "<br>\n",
    "If we assume that an attacker can have a lot of side-information, it is better to use differential privacy. For instance, we can use the Laplace mechanism, where Laplace distributed noise is added in the model. How much noise we add determines how private the result is. We can randomly chose a fraction of the data and add noise to it. This way, even if the data was publicly available, one would not be certain if the it really was true. The goal would obviously add a fraction of noise that makes the data private enough, but do not lower the predictions significantly. \n",
    "<br>\n",
    "In this task the policy is released and can be used by the public. Then the data have to be anonymized before $\\pi(a|x)$ is obtained. This can be done using a local privacy model, where independent Laplace noise $\\omega_{i}$ is added to each individual. We have $y_{i}=x_{i}+\\omega_{i}$ and use it to get $a=n^{-1}\\sum_{i=1}^{n}y_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) \n",
    "Here we assume that the analysts can be trusted with private information, so only the result made available for the public have to be privatized. Then we can use a centralized privacy model. We obtain $\\pi(a|x)$ with $a=n^{-1}\\sum_{i=1}^{n}x_{i}+\\omega$. We do not need to privatize the data, just a bit of the decisions of the model we fitted on it. \n",
    "\n",
    "In other words, we can add noise to the actions after fitting the model. Without changing any of the observations in the population, we can fit a model that decides actions, and then add a bit of noise to the actions. This is so it one could not figure out personal data based on the action we chose, which might happen if the model picks ups a simple pattern in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual implementation, we use both approaches. First, we add noise to the data itself and fit a model. Then we try to fit a model first, then add noise to the results. \n",
    "For the first approach, we implement functions for adding noise to our data. For the binary data, the function randomize() choses a ratio \"1-theta\" from a column, and changes it with a coinflip (50-50 chance of 0 and 1). For the continious variables, we replace the coinflip with a new drawing from the distribution. This approach is not as robust as desired, because generally we do not know the underlying population. We would like to change this for laplace or exponential noice by the next deadline. \n",
    "\n",
    "We then loop over all the columns in our population and add noise one by one. The model is fitted on the privatized data. \n",
    "\n",
    "\n",
    "For the second approach, we simply fit the model on the data, then sends the outcome columns to the functions to add noise. Note that the way we do this is a little too simple, we add noise the same way. This is a little bit unfortunate because we then will have individuals that do not receive treatments, and someone who recieves multiple, so we will change this later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) \n",
    "\n",
    "Let us now try to implement a policy, and see how the utility is affected by the privacy.\n",
    "The policy is a simple linear regression model, that is based on a random division of data. We first make a new population, and draw actions randomly (from the RandomPolicy class). We thereby divide into the groups which have gotten treatment 1, 2 or 3. The reward (utility for each person) is calculated, and a linear model is then fitted on each of the three groups. Finally, for each person in the new population, the three models predicts the utility, and the treatment corresponding to the model that calculates the highest utility is chosen. \n",
    "\n",
    "We have not changed the utility function, but plan to edit it so the \"features\" also affects it. If we compare two groups A and B, where A has a much higher amount of symptoms than B, but only slightly more symptoms in the outcomes, then B will have a higher utility then A. This is not satisfactory, since the treatment ratio was much better in group A. Therefore, we aim to make an utility function that is caclulated on the ratio between symptoms before and after the treatment. We do not plan to make the treatment weight in on the calculation. For now however, it is simply kept as it came with the code. \n",
    "\n",
    "The policy is very simple, so we plan to implement a method with higher predictive accuricy by the next deadline. Since we need an iterative model that is fitted on the residuals of the last model, we plan to use boosting. \n",
    "\n",
    "Here is the code for the simple policy. Please already note that we do not believe our model have any real predictive power. If we change the seed, the coefficient will change greatly, much more than the change of coefficients from group to group. We therefore believe our model is fitted on noise, or that the noise is bigger than the impact of the treatment. To not get too much noise from irrelevant variables, we simply remove the genes. We have to explore if this is a wise desicion more, but it makes the model fitting a little bit more stable. The treatment does seem to be pretty irrelevant. We will try to investigate this more later, but for now, here is what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aux_file import symptom_names\n",
    "import simulator\n",
    "from IPython import embed\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\" A policy for treatment/vaccination. \"\"\"\n",
    "    def __init__(self, n_actions, action_set):\n",
    "        \"\"\" Initialise.\n",
    "        Args:\n",
    "        n_actions (int): the number of actions\n",
    "        action_set (list): the set of actions\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.action_set = action_set\n",
    "        print(\"Initialising policy with \", n_actions, \"actions\")\n",
    "        print(\"A = {\", action_set, \"}\")\n",
    "    ## Observe the features, treatments and outcomes of one or more individuals\n",
    "    def observe(self, features, action, outcomes):\n",
    "        pass \n",
    "          \n",
    "    def get_utility(self, features, action, outcome):\n",
    "        \"\"\" Obtain the empirical utility of the policy on a set of one or more people. \n",
    "        If there are t individuals with x features, and the action\n",
    "        \n",
    "        Args:\n",
    "        features (t*|X| array)\n",
    "        actions (t*|A| array)\n",
    "        outcomes (t*|Y| array)\n",
    "        Returns:\n",
    "        Empirical utility of the policy on this data.\n",
    "      \n",
    "        Here the utiliy is defined in terms of the outcomes obtained only, ignoring both the treatment and the previous condition.\n",
    "        \"\"\"\n",
    "\n",
    "        utility = 0\n",
    "        utility -= 0.2 * sum(outcome[:,symptom_names['Covid-Positive']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Taste']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Fever']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Headache']])\n",
    "        utility -= 0.5 * sum(outcome[:,symptom_names['Pneumonia']])\n",
    "        utility -= 0.2 * sum(outcome[:,symptom_names['Stomach']])\n",
    "        utility -= 0.5 * sum(outcome[:,symptom_names['Myocarditis']])\n",
    "        utility -= 1.0 * sum(outcome[:,symptom_names['Blood-Clots']])\n",
    "        utility -= 100.0 * sum(outcome[:,symptom_names['Death']])\n",
    "        return utility\n",
    "        \n",
    "    def get_reward(self, features, actions, outcome):\n",
    "        \n",
    "        rewards = np.zeros(len(outcome))\n",
    "        for t in range(len(features)):\n",
    "            utility = 0\n",
    "            utility -= 0.2 * outcome[t,symptom_names['Covid-Positive']]\n",
    "            utility -= 0.1 * outcome[t,symptom_names['Taste']]\n",
    "            utility -= 0.1 * outcome[t,symptom_names['Fever']]\n",
    "            utility -= 0.1 * outcome[t,symptom_names['Headache']]\n",
    "            utility -= 0.5 * outcome[t,symptom_names['Pneumonia']]\n",
    "            utility -= 0.2 * outcome[t,symptom_names['Stomach']]\n",
    "            utility -= 0.5 * outcome[t,symptom_names['Myocarditis']]\n",
    "            utility -= 1.0 * outcome[t,symptom_names['Blood-Clots']]\n",
    "            utility -= 100.0 * outcome[t,symptom_names['Death']]\n",
    "            rewards[t] = utility\n",
    "        return rewards\n",
    "\n",
    "    def get_action(self, features):\n",
    "        \"\"\"Get actions for one or more people. \n",
    "        This is done by making a random policy with 3 treatments,\n",
    "        then fitting a linear model on each of the 3 subgroups.\n",
    "        The action is then calculated by which of the three models that predicts\n",
    "        the highest utility for each individual. \n",
    "        \"\"\"\n",
    "        n_population = features.shape[0]\n",
    "        model1, model2, model3 = self.linear_model(n_population)\n",
    "    \n",
    "        actions = np.zeros([n_population, self.n_actions])\n",
    "        pred1 = model1.predict(self.feature_select(features))\n",
    "        pred2 = model2.predict(self.feature_select(features))\n",
    "        pred3 = model3.predict(self.feature_select(features))\n",
    "        for t in range(n_population):\n",
    "    \n",
    "            if pred1[t] >= pred2[t] and pred1[t] >= pred3[t]:\n",
    "                actions[t, 0] = 1\n",
    "            elif pred2[t] >= pred1[t] and pred2[t] >= pred3[t]:\n",
    "                actions[t, 1] = 1\n",
    "            elif pred3[t] >= pred1[t] and pred3[t] >= pred2[t]:\n",
    "                actions[t, 2] = 1\n",
    "    \n",
    "        return actions\n",
    "    \n",
    "    def linear_model(self, n_population):\n",
    "        \"\"\"\n",
    "        Fit a linear model on random data. The data is first randomly generated\n",
    "        and a random policy is made. We then divide the data by the different\n",
    "        treatments given (which was random), and fit one linear model on each data.\n",
    "        \"\"\"\n",
    "        population = simulator.Population(128, 3, 3)\n",
    "        treatment_policy = RandomPolicy(3, list(range(3))) # make sure to add -1 for 'no vaccine'\n",
    "        X = population.generate(n_population)\n",
    "        A = treatment_policy.get_action(X)\n",
    "        U = population.treat(list(range(n_population)), A)\n",
    "        x_data = self.feature_select(X)\n",
    "        x_data1 = x_data[A[:, 0] == 1] # Action 1\n",
    "        x_data2 = x_data[A[:, 1] == 1] # Action 2\n",
    "        x_data3 = x_data[A[:, 2] == 1] # Action 3\n",
    "        y_data1 = treatment_policy.get_reward(x_data1, 0, U[A[:, 0] == 1])\n",
    "        y_data2 = treatment_policy.get_reward(x_data2, 0, U[A[:, 1] == 1])\n",
    "        y_data3 = treatment_policy.get_reward(x_data3, 0, U[A[:, 2] == 1])\n",
    "                \n",
    "        linear_model_test1 = LinearRegression()\n",
    "        linear_model_test2 = LinearRegression()\n",
    "        linear_model_test3 = LinearRegression()\n",
    "\n",
    "        model1 = linear_model_test1.fit(x_data1, y_data1)\n",
    "        model2 = linear_model_test2.fit(x_data2, y_data2)\n",
    "        model3 = linear_model_test3.fit(x_data3, y_data3)\n",
    "\n",
    "        return model1, model2, model3\n",
    "        \n",
    "    def feature_select(self, X):\n",
    "        \"\"\"\n",
    "        Chooses some columns in X. For now, we just omit the genes\n",
    "        \"\"\"\n",
    "        df = add_feature_names(X)\n",
    "        temp1 = df.iloc[:, :13]\n",
    "        temp2 = df.iloc[:, -9:-3]\n",
    "        return np.asmatrix(temp1.join(temp2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Random Policy provided with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(Policy):\n",
    "    \"\"\" This is a purely random policy!\"\"\"\n",
    "\n",
    "    def get_utility(self, features, action, outcome):\n",
    "        \"\"\"Here the utiliy is defined in terms of the outcomes obtained only, ignoring both the treatment and the previous condition.\n",
    "        \"\"\"\n",
    "        actions = self.get_action(features)\n",
    "        utility = 0\n",
    "        utility -= 0.2 * sum(outcome[:,symptom_names['Covid-Positive']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Taste']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Fever']])\n",
    "        utility -= 0.1 * sum(outcome[:,symptom_names['Headache']])\n",
    "        utility -= 0.5 * sum(outcome[:,symptom_names['Pneumonia']])\n",
    "        utility -= 0.2 * sum(outcome[:,symptom_names['Stomach']])\n",
    "        utility -= 0.5 * sum(outcome[:,symptom_names['Myocarditis']])\n",
    "        utility -= 1.0 * sum(outcome[:,symptom_names['Blood-Clots']])\n",
    "        utility -= 100.0 * sum(outcome[:,symptom_names['Death']])\n",
    "        return utility\n",
    "    \n",
    "    def get_action(self, features):\n",
    "        \"\"\"Get a completely random set of actions, but only one for each individual.\n",
    "        If there is more than one individual, feature has dimensions t*x matrix, otherwise it is an x-size array.\n",
    "        \n",
    "        It assumes a finite set of actions.\n",
    "        Returns:\n",
    "        A t*|A| array of actions\n",
    "        \"\"\"\n",
    "\n",
    "        n_people = features.shape[0]\n",
    "        ##print(\"Acting for \", n_people, \"people\");\n",
    "        actions = np.zeros([n_people, self.n_actions])\n",
    "        for t in range(features.shape[0]):\n",
    "            action = np.random.choice(self.action_set)\n",
    "            if (action >= 0):\n",
    "                actions[t,action] = 1\n",
    "            # embed()\n",
    "            \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the privitazing functions, along with some other help functions. We plan to integrate these in the class for later, but they are for now kept outside the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_names(X):\n",
    "    \"\"\"\n",
    "    This functions simply makes X to a dataframe and adds the column names, \n",
    "    so it is easier to work with.\n",
    "    \"\"\"\n",
    "    features_data = pd.DataFrame(X)\n",
    "    # features =  [\"Covid-Recovered\", \"Age\", \"Gender\", \"Income\", \"Genome\", \"Comorbidities\", \"Vaccination status\"]\n",
    "    features = []\n",
    "    # features += [\"Symptoms\" + str(i) for i in range(1, 11)]\n",
    "    features += [\"Covid-Recovered\", \"Covid-Positive\", \"No-Taste/Smell\", \"Fever\", \n",
    "                 \"Headache\", \"Pneumonia\", \"Stomach\", \"Myocarditis\", \n",
    "                 \"Blood-Clots\", \"Death\"]\n",
    "    features += [\"Age\", \"Gender\", \"Income\"]\n",
    "    features += [\"Genome\" + str(i) for i in range(1, 129)]\n",
    "    # features += [\"Comorbidities\" + str(i) for i in range(1, 7)]\n",
    "    features += [\"Asthma\", \"Obesity\", \"Smoking\", \"Diabetes\", \n",
    "                 \"Heart disease\", \"Hypertension\"]\n",
    "    features += [\"Vaccination status\" + str(i) for i in range(1, 4)]\n",
    "    features_data.columns = features\n",
    "    return features_data\n",
    "    \n",
    "def add_action_names(actions):\n",
    "    \"\"\"\n",
    "    Add names for actions. Converts array to pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(actions)\n",
    "    names = [\"Action\" + str(i) for i in range(1, len(actions.shape[0]) + 1)]\n",
    "    df.columns = names\n",
    "    return df\n",
    "\n",
    "def add_outcome_names(outcomes):\n",
    "    \"\"\"\n",
    "    Add names for the outcomes. Converts array to pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(outcomes)\n",
    "    df.columns = [\"Covid-Recovered\", \"Covid-Positive\", \"No-Taste/Smell\", \"Fever\", \n",
    "                  \"Headache\", \"Pneumonia\", \"Stomach\", \"Myocarditis\", \n",
    "                  \"Blood-Clots\", \"Death\"]\n",
    "    return df\n",
    "    \n",
    "def privatize(X, theta):\n",
    "    \"\"\"\n",
    "    Adds noice to the data, column by column. The continious and discreet \n",
    "    columns are treated differently. \n",
    "    \"\"\"\n",
    "    df = add_feature_names(X).copy()\n",
    "    df[\"Age\"] = randomize_age(df[\"Age\"], theta)\n",
    "    df[\"Income\"] = randomize_income(df[\"Income\"], theta)\n",
    "    for column in df.columns:\n",
    "        if column != \"Age\" or column != \"Income\":\n",
    "            df[column] = randomize(df[column], theta)\n",
    "    return np.asarray(df)\n",
    "\n",
    "def privatize_actions(A, theta):\n",
    "    \"\"\"\n",
    "    Adds noise to the actions chosen bu the model. This is currently done\n",
    "    a little bit primitive, since person no longer receives exactly one\n",
    "    treatment.\n",
    "    \"\"\"\n",
    "    A1 = A.copy()\n",
    "    for i in range(A1.shape[1]):\n",
    "        A1[:, i] = randomize(A1[:, i], theta)\n",
    "    return A1\n",
    "    \n",
    "def randomize(a, theta):\n",
    "    \"\"\"\n",
    "    Randomize a single column. Simply add a cointoss to \"theta\" amount of the data\n",
    "    \"\"\"\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=a.shape)\n",
    "    noise = np.random.choice([0, 1], size=a.shape)\n",
    "    response = np.array(a)\n",
    "    response[~coins] = noise[~coins]\n",
    "    return response \n",
    "    \n",
    "def randomize_income(a, theta):\n",
    "    \"\"\"\n",
    "    Randomize by drawing from the same population again\n",
    "    \"\"\"\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=a.shape)\n",
    "    noise = np.random.gamma(1,10000, size=a.shape)\n",
    "    response = np.array(a)\n",
    "    response[~coins] = noise[~coins]\n",
    "    return response \n",
    "    \n",
    "def randomize_age(a, theta):\n",
    "    \"\"\"\n",
    "    Randomize by drawing from the same population again\n",
    "    \"\"\"\n",
    "    coins = np.random.choice([True, False], p=(theta, (1-theta)), size=a.shape)\n",
    "    noise = np.random.gamma(3,11, size=a.shape)\n",
    "    response = np.array(a)\n",
    "    response[~coins] = noise[~coins]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "\n",
    "Let us try to explore what the privatizing results in. We first see what happens when we add some noise to the data before we fit the model. We add different amount of noise, with theta values in [0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5]. This takes a couple seconds to run, and excuse the messy output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n",
      "0\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(57)\n",
    "n_genes = 128\n",
    "n_vaccines = 3\n",
    "n_treatments = 3\n",
    "n_population = 10000\n",
    "population = simulator.Population(n_genes, n_vaccines, n_treatments)\n",
    "treatment_policy = Policy(n_treatments, list(range(n_treatments)))\n",
    "X = population.generate(n_population)\n",
    "np.random.seed(57)\n",
    "A = treatment_policy.get_action(X)\n",
    "np.random.seed(57)\n",
    "U = population.treat(list(range(n_population)), A)\n",
    "utility = treatment_policy.get_utility(X, A, U)\n",
    "\n",
    "thetas = [0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "utility_list = np.zeros(len(thetas)+1)\n",
    "utility_list[0] = utility\n",
    "for i in range(len(thetas)):\n",
    "    print(i)\n",
    "    X_priv = privatize(X, thetas[i])\n",
    "    np.random.seed(57)\n",
    "    A_priv = treatment_policy.get_action(X_priv)\n",
    "    np.random.seed(57)\n",
    "    U_priv = population.treat(list(range(n_population)), A_priv)\n",
    "    utility_list[i+1] = treatment_policy.get_utility(X_priv, A_priv, U_priv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1914.2, -1914.9, -1916.7, -1920.8, -1926.4, -1929.7, -1831.7,\n",
       "       -2033.4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discuss the results. We first get a little but lower utility as we add more noise (lower theta). This seems to be in line with our model. However, when we add a lot of noise, the utility changes by a lot, and even gets higher. We assume that this is because the model is fitted on a lot of noise, so it is not actually explaining much. Simply randomizing all the data might give a higher utility at some time. However, the first 5 theta values are gradually increasing, but not by much, which is in line with the theory. This means that an adition in privacy reduces the prediction by a little amount. This reduction is so little that it can be absolutely worth to change for privacy. However, if the seed is changed, the values might as well. One would have to run this on many seeds, but that takes time, and we wish to do a more detailed analysis on a better model in rather than this. \n",
    "\n",
    "Let us look at what happens when we add noise to the actions, and not the population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_list2 = np.zeros(len(thetas) + 1)\n",
    "utility_list2[0] = utility\n",
    "for i in range(len(thetas)):\n",
    "    np.random.seed(57)\n",
    "    A_noise = privatize_actions(A, thetas[i])\n",
    "    U_noise = population.treat(list(range(n_population)), A_noise)\n",
    "    utility_list2[i+1] = treatment_policy.get_utility(X, A_noise, U_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1914.2, -2512.3, -2511.9, -2513.2, -2511.4, -2414.8, -2118.7,\n",
       "       -2018.2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the last part, it is hard to trust the results, but adding a bit of noise significantly changed the utility to the worse. Here it seems like the first approach was better. However, we will not look more into this until we have refined our utility function and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "We have some questions about the assigmnet that we hope to get some guidance on, before the next deadline. \n",
    "\n",
    "1. How is the model meant to be increamantably updated? We assume that we start with a simple model, calculate the utility, then use stochastic gradiant descent to find out what to update in our model. However, how is this done? Which kind of model can we update in this way? We wanted to use boosting, since it is an iterative process of additave models fit on the residuals, but we do not know how to actually find a library that implements it in a way with the utility function like this. \n",
    "\n",
    "2. What is meant to be in the observe() function? Is this what we did in the reward instead, or is it different?\n",
    "\n",
    "3. When will the \"historical data\" for the last deadline be available? Is it sufficient to calculate the error bounds with bootstrapping? How is the \"improved policy\" meant to be found, simply by using the new data, or finding a better model?\n",
    "\n",
    "4. The code from the population-generation produces some warnings. How can we get rid of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
