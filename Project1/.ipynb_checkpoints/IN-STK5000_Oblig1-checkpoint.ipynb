{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import binom\n",
    "np.random.seed(57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## a)\n",
    "In this section, we are going to explore if the genes, age and comorbidities can predict any of the symptoms. The method we are going to use is a simple correlation check. The method will be briefly presented underneath, while the specifics will be presented with the code shortly afterwards. \n",
    "\n",
    "We are exploring which of the explanatory variables (age, genes and comorbidities), which has a high correlation with each of the responses (symptoms). We check all of these correlations and chose our variables according to a correlation-threshold. We chose our threshold in advance, based some calculation. Given that the columns are independent with the response, we see which maximum absolute value of the correlation is expected to get in most cases. We then test our proceduere on synthetic data. This data is produces so that we know which columns that are correlated to the response, and the rest are random. We then test to see if our selection methods works at this data, and then test it on the actual data.\n",
    "\n",
    "For this approach we assume that if a variable is related to a response, then the correlation will be big. The drawbacks of this assumption is that we can have correlation between a bigger set of variables and the response, but not each of the variables independent. However, testing this requires more computing power.\n",
    "\n",
    "Now, let's get into the code and the details. Firstly, we made a function for reading the data and naming the columns accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_features(data):\n",
    "        \"\"\"\n",
    "        Initialize names for observation features and treatment features\n",
    "        \"\"\"\n",
    "        features_data = pd.read_csv(data)\n",
    "        features = []\n",
    "        features += [\"Covid-Recovered\", \"Covid-Positive\",\n",
    "            \"No-Taste/Smell\",  \"Fever\", \"Headache\", \"Pneumonia\",\n",
    "            \"Stomach\", \"Myocarditis\", \"Blood-Clots\", \"Death\"]\n",
    "        features += [\"Age\", \"Gender\", \"Income\"]\n",
    "        features += [\"Genome\" + str(i) for i in range(1, 129)]\n",
    "        features += [\"Asthma\", \"Obesity\", \"Smoking\", \"Diabetes\", \n",
    "                     \"Heart disease\", \"Hypertension\"]\n",
    "        features += [\"Vaccination status\" + str(i) for i in range(1, 4)]\n",
    "        features_data.columns = features\n",
    "        return features_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also made some functions for calculating the correlation, and the subset-selection based on the correlation. We simply test all of the columns in the dataframe against the responses and select them iff they have a correlation higher than some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(col1, col2):\n",
    "    \"\"\"\n",
    "    Calculates the correlation (pearson correlation) between col1 and col2.\n",
    "    Cor(X, Y) = Sum (x_i - mu_x) (y_i - mu_y) / (std(X) * std(Y) * n)\n",
    "    Divides by n and not (n-1), as some functions do. \n",
    "    \"\"\"\n",
    "    mean1 = np.mean(col1)\n",
    "    mean2 = np.mean(col2)\n",
    "    sum = 0\n",
    "    for i in range(len(col1)):\n",
    "        sum += (col1[i] - mean1) * (col2[i] - mean2)\n",
    "    cor = sum / (np.std(col1) * np.std(col2) * len(col1))\n",
    "    return cor\n",
    "\n",
    "def correlation_select(data, response, correlation_threshold=0.01):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        data (np.array): ((m, n)) sized array of explanatory variables.\n",
    "        response (np.array): (m) sized array of the response.\n",
    "        correlation_threshold (int): Threshold for when the correlation is high\n",
    "            enough for variable to be chosen.\n",
    "    Out:\n",
    "        selected_columns (list): List of the indexes of the columns that are\n",
    "            chosen, with the corresponding correlation. [[1, cor1], [2, cor2], ... ]\n",
    "            \n",
    "    Feature selection based on univariate correlation between a column and the\n",
    "    response. Looks at each column in \"data\" independetly and calculates\n",
    "    the correlation between it and the response. Iff it is over \n",
    "    \"correlation_threshold\" it is chosen. \n",
    "    \"\"\"\n",
    "    selected_columns = []\n",
    "    data = data.to_numpy() # This runs a bit faster\n",
    "    for i in range(data.shape[1]):\n",
    "        cor = correlation(response, data[:, i])\n",
    "        if abs(cor) > correlation_threshold:\n",
    "            selected_columns.append([i, cor])\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore what to use as a threshold for the subset-selection. The dataset we are exploring have 100000 rows and about 150 columns. Even if the columns are drawn from a distribution independent from the response, the calculated correlation will still be slightly hihger than 0. Therefore, we wish to find out which correlation that is _very little likely_ not to encounter by random data. Hypothesis tests ofthen have confidence intervals defined by the 95 or 99 percentile, but we do not consider this to be sufficient. Since we have over 100 observations, if we where to set the threshold according to the correlation corresponding to something that is 1% or less likely to encounter from random data, we would still expect to chose one column just at random. Therefore, our threshold needs to be stricter. We concluded that we wanted to look at a correlation only 0.1% of random columns would have, altough the exact number 0.1% was arbritralely chosen.  \n",
    "\n",
    "Now we need to find out how high correlation the most correlated 0.1% of random data is expected to have. We assume that the response and variables are binary with means approximately 0.5. The explanatory variable has a 50% chance of being the same as the response, for each of the rows. With the help of the cumulative function for binomial data, we find the amount of similar inputs to expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010022415200593084\n"
     ]
    }
   ],
   "source": [
    "print(binom.cdf(k=49511, n=100000, p=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, in the random case, the most extreme 0.1% of the columns have 100000-49511 = 50489 similar rows. Now we calculate the correlation this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009780467754231225"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000\n",
    "k = 49511\n",
    "col1 = np.zeros(n)\n",
    "col2 = np.zeros(n) \n",
    "# I want the mean to be close to 0.5, and columns equal in n - k inputs. \n",
    "for i in range(int(n/2)):\n",
    "    col1[i] = 1\n",
    "    col2[2*i] = 1\n",
    "for i in range(int(n/2 - k)):\n",
    "    col1[2*i] = 0\n",
    "\n",
    "diff = abs(col1-col2)\n",
    "correlation(col1, col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and a little more safity, we round up to an absolute value of 0.01.\n",
    "\n",
    "Not that there are some weaknesess to our method. The mean is not exacly 0.5, and the calculation will therefore be a little bit off. However, the 0.1% chosen is pretty safe, so if we find correlation above 0.01, it is not very likely to be completely random. \n",
    "\n",
    "Now we can preceed with the actual selection, first by testing on correlated data. We made our own data generator. It takes some input about the dimensions of the data, how many columns to be correlated, and how correlated they should be. The response is created with random binary inputs and a expected mean of 0.5. The comments should make the code pretty readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlated_data(num_col, num_cor, num_row, prob=0.5):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        num_col (int): Number of columns in the matrix\n",
    "        num_cor (int): Number of the columns that should be correlated with \n",
    "            the response. num_cor <= num_col.\n",
    "        num_row (int): Number of observations.\n",
    "        prob (float): Probability for correlated columns to be equal to the\n",
    "            response. If not, value is random.\n",
    "    Out:\n",
    "        data (pd.DataFrame): ((num_row, num_col)) matrix of data, where the \n",
    "            num_cor first columns are correlated with the response.\n",
    "        response (pd.Series): (num_row) size series of the response, which is\n",
    "            randomly chosen 0 or 1 for each input. \n",
    "            \n",
    "    Creates correlated data. The response is randomly chosen 0 or 1 with a\n",
    "    probability of 0.5. Then a matrix of size (num_row, num_col) is created, \n",
    "    where the first num_cor columns are correlated with the response, and the \n",
    "    rest (num_col - num_cor) is randomly generated. \n",
    "    \n",
    "    For each of the correlated columns, they are chosen equal to the response\n",
    "    with a probability of \"prob\". If not, they are randomly chosen 0 or 1 with \n",
    "    a probability of 0.5. \n",
    "    \"\"\"\n",
    "    response = np.random.randint(2, size=num_row) # Random response\n",
    "    data = np.zeros((num_row, num_col))\n",
    "    for i in range(num_cor): # Fill in value for matrix\n",
    "        for j in range(num_row):\n",
    "            coin_flip = np.random.uniform()\n",
    "            if coin_flip < prob: \n",
    "                data[j, i] = response[j] # Correlated column sets equal to response\n",
    "            else: \n",
    "                coin_flip = np.random.uniform() # Correlated column is set random\n",
    "                if coin_flip < 0.5:\n",
    "                    data[j, i] = 0\n",
    "                else:\n",
    "                    data[j, i] = 1\n",
    "    for i in range(num_cor, num_col): # The rest of the columns are random\n",
    "        data[:, i] = np.random.randint(2, size=num_row)\n",
    "    return pd.DataFrame(data), pd.Series(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual experiment. We make a dataset of 100000 rows, 150 columns and the probability for the data generator to be 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, response = create_correlated_data(150, 10, 100000, prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list = correlation_select(data, response, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(cor_list)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our selection chose the 10 first columns, and nothing else, exactly as we wanted. Let us now try to run the method on the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = init_features(\"observation_features.csv\")\n",
    "genomes = data.iloc[:, 13:141] # Columns corresponding to Genomes\n",
    "age = data.iloc[:, 10] # Age\n",
    "comorbidities = data.iloc[:, 141:147] # All of comorbidities\n",
    "symptoms = data.iloc[:, :10]\n",
    "vaccines = data.iloc[:, -3:]\n",
    "df = pd.DataFrame(age).join(genomes.join(comorbidities))\n",
    "responses = symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code takes a little while to load, ~5 minutes. This would be much faster if we vectorized the functions, but we did not consider this important for this assignement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "for i in range(len(symptoms.columns)):\n",
    "    correlation_list.append(correlation_select(df, responses.iloc[:, i], 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "We will now explore the efficacy of the vaccines. We formalize this problem as looking at two things: Given that group has either got a vaccine or not, how likely are they to 1 - get Covid-Positive, and 2 - If they are Covid-Positive, how like likely are they to die. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_status = data.iloc[:, -3:] # Columns corresponding to vaccines\n",
    "vaccines = vaccine_status.join(symptoms)\n",
    "\n",
    "# Not that vaccine1, vaccine2 and vaccine3 are disjunct sets. \n",
    "vaccine1 = vaccines[vaccines[\"Vaccination status1\"] == 1] # Taken first vaccine\n",
    "vaccine2 = vaccines[vaccines[\"Vaccination status2\"] == 1]\n",
    "vaccine3 = vaccines[vaccines[\"Vaccination status3\"] == 1]\n",
    "no_vaccine = vaccines[(vaccines[\"Vaccination status1\"] == 0.0) &\n",
    "                      (vaccines[\"Vaccination status2\"] == 0.0) &\n",
    "                      (vaccines[\"Vaccination status3\"] == 0.0)]\n",
    "any_vaccine = vaccines[(vaccines[\"Vaccination status1\"] == 1) |\n",
    "                       (vaccines[\"Vaccination status2\"] == 1) |\n",
    "                       (vaccines[\"Vaccination status3\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaccine_sick_test(df1, df2):\n",
    "    \"\"\"\n",
    "    Hypothesis test on df1 and df2 if you are more probable to get Covid in \n",
    "    group df1 and df2.\n",
    "    \"\"\"\n",
    "    n1 = len(df1) # Number of people in each population\n",
    "    n2 = len(df2)\n",
    "    s1 = len(df1[df1[\"Covid-Positive\"] == 1])\n",
    "    s2 = len(df2[df2[\"Covid-Positive\"] == 1])\n",
    "    p1 = s1/n1 # Ratio of Covid-Positives over whole group\n",
    "    p2 = s2/n2\n",
    "    p_hat = (s1 + s2)/(n1 + n2) # Parameter for test statistic\n",
    "    z_value = (p1 - p2)/np.sqrt((p_hat*(1-p_hat)*(1/n1 + 1/n2)))\n",
    "    return p1, p2, z_value\n",
    "\n",
    "def vaccine_death_test(df1, df2):\n",
    "    \"\"\"\n",
    "    Hypothesis test on if you are more likely to die from covid in df1 than df2.\n",
    "    We only look at people with covid in both groups.\n",
    "    \"\"\"\n",
    "    # n1 = len(df1)\n",
    "    # n2 = len(df2)\n",
    "    n1 = len(df1[df1[\"Covid-Positive\"] == 1]) # Number of people in each population\n",
    "    n2 = len(df2[df2[\"Covid-Positive\"] == 1])\n",
    "    s1 = len(df1[(df1[\"Covid-Positive\"] == 1) & (df1[\"Death\"] == 1)]) \n",
    "    s2 = len(df2[(df2[\"Covid-Positive\"] == 1) & (df2[\"Death\"] == 1)])\n",
    "    p1 = s1/n1 # Ratio of Covid-Deaths over whole group\n",
    "    p2 = s2/n2\n",
    "    p_hat = (s1 + s2)/(n1 + n2) # Parameter for test statistic\n",
    "    z_value = (p1 - p2)/np.sqrt((p_hat*(1-p_hat)*(1/n1 + 1/n2)))\n",
    "    return p1, p2, z_value\n",
    "\n",
    "def get_df_name(df):\n",
    "    \"\"\"\n",
    "    Getting name of a dataframe.\n",
    "    \"\"\"\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [any_vaccine, vaccine1, vaccine2, vaccine3]\n",
    "\n",
    "for group in groups:\n",
    "    p1_s, p2_s, z_s = vaccine_sick_test(group, no_vaccine)\n",
    "    p1_d, p2_d, z_d = vaccine_death_test(group, no_vaccine)\n",
    "    print()\n",
    "    print(f\"Testing group {get_df_name(group)} against non-vaccinated\")\n",
    "    print(f\"Percentages of people who got covid:\")\n",
    "    print(f\"    Vaccinated: {p1_s*100:.4f}, Unvaccinated: {p2_s*100:.4f}\")\n",
    "    print(f\"    z_value: {z_s}\")\n",
    "    print(f\"Percentages of Covid-Positive persons that died:\")\n",
    "    print(f\"    Vaccinated: {p1_d*100:.4f}, Unvaccinated: {p2_d*100:.4f}\")\n",
    "    print(f\"    z_value: {z_d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "\n",
    "For this section, we will explore if the vaccines are likely to cause any side effects. We formalize this statement as how likely one are to _not_ be Covid-Positive, but to have one of the other symptoms. We look at the group that is not vaccinated, and hypothesis test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_effect_test(df1, df2, symptom):\n",
    "    \"\"\"\n",
    "    In: \n",
    "        df1: (df) DataFrame of population1\n",
    "        df2: (df) DataFrame of population2\n",
    "        symptom: (str) Column name corresponding to the symptom to be tested.\n",
    "    Out:\n",
    "        p1: (float) Ratio of non-Covid-Positive people that have symptoms in df1\n",
    "        p2: (float) Ratio of non-Covid-Positive people that have symptoms in df1\n",
    "        z_value: (float) z-value according to the hypothesis test (see below). \n",
    "    Tests if there is a significant increase of propability to get the \n",
    "    \"symptom\" as a side effect in df1 than df2, or vice verca. Side effect\n",
    "    is defined as having a symptom, but not being Covid-Positive. \n",
    "    \n",
    "    Hypothesis test (approximated standard normal for binomial data, page 521\n",
    "    in \"Modern Mathematical Statistics with Applications, Devore, Berk\". \n",
    "    z = (p1 - p2)/sqrt(p_hat (1-p_hat) (1/n + 1/m))\n",
    "    where p1 and p2 is ratio between positives and size of sample 1 and 2, \n",
    "    respectivly, n and m are the size of sample 1 and 2, respectivly, and \n",
    "    p_hat is (X + Y)/(n + m), where X and Y are the positives in sample 1 and 2, \n",
    "    once again respectivly. \n",
    "    \"\"\"\n",
    "    sample1 = df1[df1[\"Covid-Positive\"] == 0] \n",
    "    sample2 = df2[df2[\"Covid-Positive\"] == 0]\n",
    "    n1 = len(df1) # Number of people in each population\n",
    "    n2 = len(df2)\n",
    "    s1 = len(sample1[sample1[symptom] == 1]) # Amount of people having the symptom\n",
    "    s2 = len(sample2[sample2[symptom] == 1])\n",
    "    p1 = s1/n1 # Ratio of symptomatic people and non-symptomatic\n",
    "    p2 = s2/n2\n",
    "    p_hat = (s1 + s2)/(n1 + n2) # Parameter for test statistic\n",
    "    z_value = (p1 - p2)/np.sqrt((p_hat*(1-p_hat)*(1/n1 + 1/n2)))\n",
    "    return p1, p2, z_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Side effects\n",
    "symptom_names = [\"No-Taste/Smell\", \"Fever\", \"Headache\", \"Pneumonia\", \"Stomach\", \n",
    "                 \"Myocarditis\", \"Blood-Clots\", \"Death\"]\n",
    "for symptom in symptom_names: \n",
    "    p1, p2, z_value = side_effect_test(any_vaccine, no_vaccine, symptom)\n",
    "    print(f\"Symptom {symptom}: Vaccinated {p1*100:.4f}, unvaccinated: {p2*100:.4f}\")\n",
    "    print(f\"    The z-value is {z_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
