{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import binom\n",
    "np.random.seed(57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## a)\n",
    "In this section, we are going to explore if the genes, age and comorbidities can predict any of the symptoms. The method we are going to use is a simple correlation check. The method will be briefly presented underneath, while the specifics will be presented with the code shortly afterwards. \n",
    "\n",
    "We are exploring which of the explanatory variables (age, genes and comorbidities), which has a high correlation with each of the responses (symptoms). We check all of these correlations and chose our variables according to a correlation-threshold. We chose our threshold in advance, based some calculation. Given that the columns are independent with the response, we see which maximum absolute value of the correlation is expected to get in most cases. We then test our proceduere on synthetic data. This data is produces so that we know which columns that are correlated to the response, and the rest are random. We then test to see if our selection methods works at this data, and then test it on the actual data.\n",
    "\n",
    "For this approach we assume that if a variable is related to a response, then the correlation will be big. The drawbacks of this assumption is that we can have correlation between a bigger set of variables and the response, but not each of the variables independent. However, testing this requires more computing power.\n",
    "\n",
    "Now, let's get into the code and the details. Firstly, we made a function for reading the data and naming the columns accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_features(data):\n",
    "        \"\"\"\n",
    "        Initialize names for observation features and treatment features\n",
    "        \"\"\"\n",
    "        features_data = pd.read_csv(data)\n",
    "        features = []\n",
    "        features += [\"Covid-Recovered\", \"Covid-Positive\",\n",
    "            \"No-Taste/Smell\",  \"Fever\", \"Headache\", \"Pneumonia\",\n",
    "            \"Stomach\", \"Myocarditis\", \"Blood-Clots\", \"Death\"]\n",
    "        features += [\"Age\", \"Gender\", \"Income\"]\n",
    "        features += [\"Genome\" + str(i) for i in range(1, 129)]\n",
    "        features += [\"Asthma\", \"Obesity\", \"Smoking\", \"Diabetes\", \n",
    "                     \"Heart disease\", \"Hypertension\"]\n",
    "        features += [\"Vaccination status\" + str(i) for i in range(1, 4)]\n",
    "        features_data.columns = features\n",
    "        return features_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also made some functions for calculating the correlation, and the subset-selection based on the correlation. We simply test all of the columns in the dataframe against the responses and select them iff they have a correlation higher than some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(col1, col2):\n",
    "    \"\"\"\n",
    "    Calculates the correlation (pearson correlation) between col1 and col2.\n",
    "    Cor(X, Y) = Sum (x_i - mu_x) (y_i - mu_y) / (std(X) * std(Y) * n)\n",
    "    Divides by n and not (n-1), as some functions do. \n",
    "    \"\"\"\n",
    "    mean1 = np.mean(col1)\n",
    "    mean2 = np.mean(col2)\n",
    "    sum = 0\n",
    "    for i in range(len(col1)):\n",
    "        sum += (col1[i] - mean1) * (col2[i] - mean2)\n",
    "    cor = sum / (np.std(col1) * np.std(col2) * len(col1))\n",
    "    return cor\n",
    "\n",
    "def correlation_select(data, response, correlation_threshold=0.01):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        data (np.array): ((m, n)) sized array of explanatory variables.\n",
    "        response (np.array): (m) sized array of the response.\n",
    "        correlation_threshold (int): Threshold for when the correlation is high\n",
    "            enough for variable to be chosen.\n",
    "    Out:\n",
    "        selected_columns (list): List of the indexes of the columns that are\n",
    "            chosen, with the corresponding correlation. [[1, cor1], [2, cor2], ... ]\n",
    "            \n",
    "    Feature selection based on univariate correlation between a column and the\n",
    "    response. Looks at each column in \"data\" independetly and calculates\n",
    "    the correlation between it and the response. Iff it is over \n",
    "    \"correlation_threshold\" it is chosen. \n",
    "    \"\"\"\n",
    "    selected_columns = []\n",
    "    data = data.to_numpy() # This runs a bit faster\n",
    "    for i in range(data.shape[1]):\n",
    "        # cor = correlation(response, data[:, i])\n",
    "        cor = np.corrcoef(response, data[:, i])[1, 0] # Correlation, vectorized\n",
    "        if abs(cor) > correlation_threshold:\n",
    "            selected_columns.append([i, cor])\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore what to use as a threshold for the subset-selection. The dataset we are exploring have 100000 rows and about 150 columns. Even if the columns are drawn from a distribution independent from the response, the calculated correlation will still be slightly hihger than 0. Therefore, we wish to find out which correlation that is _very little likely_ not to encounter by random data. Hypothesis tests ofthen have confidence intervals defined by the 95 or 99 percentile, but we do not consider this to be sufficient. Since we have over 100 observations, if we where to set the threshold according to the correlation corresponding to something that is 1% or less likely to encounter from random data, we would still expect to chose one column just at random. Therefore, our threshold needs to be stricter. We concluded that we wanted to look at a correlation only 0.1% of random columns would have, altough the exact number 0.1% was arbritralely chosen.  \n",
    "\n",
    "Now we need to find out how high correlation the most correlated 0.1% of random data is expected to have. We assume that the response and variables are binary with means approximately 0.5. The explanatory variable has a 50% chance of being the same as the response, for each of the rows. With the help of the cumulative function for binomial data, we find the amount of similar inputs to expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010022415200593084\n"
     ]
    }
   ],
   "source": [
    "print(binom.cdf(k=49511, n=100000, p=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, in the random case, the most extreme 0.1% of the columns have 100000-49511 = 50489 similar rows. Now we calculate the correlation this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009780467754231225"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000\n",
    "k = 49511\n",
    "col1 = np.zeros(n)\n",
    "col2 = np.zeros(n) \n",
    "# I want the mean to be close to 0.5, and columns equal in n - k inputs. \n",
    "for i in range(int(n/2)):\n",
    "    col1[i] = 1\n",
    "    col2[2*i] = 1\n",
    "for i in range(int(n/2 - k)):\n",
    "    col1[2*i] = 0\n",
    "\n",
    "diff = abs(col1-col2)\n",
    "correlation(col1, col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and a little more safity, we round up to an absolute value of 0.01.\n",
    "\n",
    "Not that there are some weaknesess to our method. The mean is not exacly 0.5, and the calculation will therefore be a little bit off. However, the 0.1% chosen is pretty safe, so if we find correlation above 0.01, it is not very likely to be completely random. \n",
    "\n",
    "In our data, the mean is far from 0.5. Let us see what it actually is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = init_features(\"observation_features.csv\") # Initialization of data\n",
    "genomes = data.iloc[:, 13:141] # Columns corresponding to Genomes\n",
    "age = data.iloc[:, 10] # Age\n",
    "comorbidities = data.iloc[:, 141:147] # All of comorbidities\n",
    "symptoms = data.iloc[:, :10]\n",
    "vaccines = data.iloc[:, -3:]\n",
    "df = pd.DataFrame(age).join(genomes.join(comorbidities))\n",
    "responses = symptoms\n",
    "vaccine_status = data.iloc[:, -3:] # Columns corresponding to vaccines\n",
    "vaccines = vaccine_status.join(symptoms) # For 1b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: 33.0295\n",
      "Genome1: 0.5013\n",
      "Genome2: 0.5007\n",
      "Genome3: 0.5007\n",
      "Genome4: 0.5005\n",
      "Genome5: 0.5013\n",
      "Genome6: 0.4984\n",
      "Genome7: 0.5024\n",
      "Genome8: 0.4985\n",
      "Genome9: 0.4987\n",
      "Covid-Recovered: 0.0491\n",
      "Covid-Positive: 0.2203\n",
      "No-Taste/Smell: 0.0141\n",
      "Fever: 0.0585\n",
      "Headache: 0.0321\n",
      "Pneumonia: 0.0094\n",
      "Stomach: 0.0028\n",
      "Myocarditis: 0.0036\n",
      "Blood-Clots: 0.0099\n",
      "Death: 0.0033\n"
     ]
    }
   ],
   "source": [
    "    for i in range(10):\n",
    "        print(f\"{df.columns[i]}: {sum(df.iloc[:, i])/len(df):.4f}\")\n",
    "    for symptom in symptoms.columns:\n",
    "        print(f\"{symptom}: {sum(symptoms[symptom])/len(symptoms):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not spam the output more than I already have, I do not print all of the variables. However, we do seem a definite trend if we do. The Genes seems to be centered around mean 0.5. The comorbidieties do not, and definitely not the symptoms. This will make the correlation higher for columns that are similar in 50489 rows. It is much we could have checked, but let's try one column that have mean 0.5 (representing a gene) and one with mean around 0.25 (representing the Covid-Positive column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50489\n",
      "-0.016940267072052973\n"
     ]
    }
   ],
   "source": [
    "col1 = np.zeros(n)\n",
    "col2 = np.zeros(n)\n",
    "for i in range(int(n/2)):\n",
    "    col1[i] = 1\n",
    "for i in range(int(n/4)):\n",
    "    col2[i*4] = 1\n",
    "\n",
    "for i in range(int(n/2 - k)):\n",
    "    col1[4*i] = 0\n",
    "\n",
    "diff = abs(col1-col2)\n",
    "print(np.mean(diff))\n",
    "# 0.50489\n",
    "print(correlation(col1, col2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the absolute value of the correlation is higher, about 0.017. With the columns that have means even further away from 0.5 it will be even higher, but I will use 0.017 to be sure to get all of the columns (we will see later that there is not really any that meets even this requirement, so a higher threshold would not change much). However, for the synthetic data our means are 0.5, so here I will use 0.01 to confirm that our method works.\n",
    "\n",
    "Now we can preceed with the actual selection, first by testing on correlated data. We made our own data generator. It takes some input about the dimensions of the data, how many columns to be correlated, and how correlated they should be. The response is created with random binary inputs and a expected mean of 0.5. The comments should make the code pretty readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlated_data(num_col, num_cor, num_row, prob=0.5):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        num_col (int): Number of columns in the matrix\n",
    "        num_cor (int): Number of the columns that should be correlated with \n",
    "            the response. num_cor <= num_col.\n",
    "        num_row (int): Number of observations.\n",
    "        prob (float): Probability for correlated columns to be equal to the\n",
    "            response. If not, value is random.\n",
    "    Out:\n",
    "        data (pd.DataFrame): ((num_row, num_col)) matrix of data, where the \n",
    "            num_cor first columns are correlated with the response.\n",
    "        response (pd.Series): (num_row) size series of the response, which is\n",
    "            randomly chosen 0 or 1 for each input. \n",
    "            \n",
    "    Creates correlated data. The response is randomly chosen 0 or 1 with a\n",
    "    probability of 0.5. Then a matrix of size (num_row, num_col) is created, \n",
    "    where the first num_cor columns are correlated with the response, and the \n",
    "    rest (num_col - num_cor) is randomly generated. \n",
    "    \n",
    "    For each of the correlated columns, they are chosen equal to the response\n",
    "    with a probability of \"prob\". If not, they are randomly chosen 0 or 1 with \n",
    "    a probability of 0.5. \n",
    "    \"\"\"\n",
    "    response = np.random.randint(2, size=num_row) # Random response\n",
    "    data = np.zeros((num_row, num_col))\n",
    "    for i in range(num_cor): # Fill in value for matrix\n",
    "        for j in range(num_row):\n",
    "            coin_flip = np.random.uniform()\n",
    "            if coin_flip < prob: \n",
    "                data[j, i] = response[j] # Correlated column sets equal to response\n",
    "            else: \n",
    "                coin_flip = np.random.uniform() # Correlated column is set random\n",
    "                if coin_flip < 0.5:\n",
    "                    data[j, i] = 0\n",
    "                else:\n",
    "                    data[j, i] = 1\n",
    "    for i in range(num_cor, num_col): # The rest of the columns are random\n",
    "        data[:, i] = np.random.randint(2, size=num_row)\n",
    "    return pd.DataFrame(data), pd.Series(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual experiment. We make a dataset of 100000 rows, 150 columns and the probability for the data generator to be 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, response = create_correlated_data(150, 10, 100000, prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list = correlation_select(data, response, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(cor_list)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our selection chose the 10 first columns, and nothing else, exactly as we wanted. Let us now try to run the method on the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "for i in range(len(symptoms.columns)):\n",
    "    # print(f\"Symptom: {responses.columns[i]}\")\n",
    "    correlation_list.append(correlation_select(df, responses.iloc[:, i], 0.017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_list # Make output nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for 1a)\n",
    "\n",
    "As we see, none of the symptoms have 'high enough' correlation to be relevant, according to our last definition of relevant. If one runs the test with 0.01 instead of 0.017, one would see that about 10 variables have correlation between 0.01 and 0.015. By this method, we conclude that age, genes and comorbidities do not predict the symptoms in any meaningful way. There is some correlation, but we consider this noise. If we were to just strictly predict, and not try to explain, then we might still have used some of the variables. In that case, we would not care about the statistical insignificance, just if our test-error got lower. Using some of the variables would still probably be better than guessing, but not by so much that it would be enough to trust the variables for haveing explanatory power. \n",
    "\n",
    "The big drawback here is that we only test one variable at the time. We might expect a cluster of genes to have predictive power, not an individual gene. However, we were not able to find that either. Although, since this is a big drawback, we do not consider this method to rule out the possibility of the genes being relevant to the symptoms. We do however have some evidence towards genes (and comorbidities) being seemingly random compared to the symptoms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "We will now explore the efficacy of the vaccines. The patients that are vaccinated will be tested against those who are not, but we will also test the three vaccines individually. We will look at two things: How likely are a patient to be Covid-Positive, given that they have gotten a vaccine or not? 2. How likely are a person to die from Covid (both be Covid-Positive and experience Death), given that they have gotten the vaccine or not? This is formalized as a binomial hypothesis test, which is approximated standard normal with big data sets (groups are both bigger than 40, as we have here). The problem is then to test if there is statistic significance between the vaccinated group and the non-vaccinated group, and if so, is how big is the difference (is it clinicly significant?). \n",
    "\n",
    "We start by initializing the data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not that vaccine1, vaccine2 and vaccine3 are disjunct sets. \n",
    "vaccine1 = vaccines[vaccines[\"Vaccination status1\"] == 1] # Taken first vaccine\n",
    "vaccine2 = vaccines[vaccines[\"Vaccination status2\"] == 1]\n",
    "vaccine3 = vaccines[vaccines[\"Vaccination status3\"] == 1]\n",
    "no_vaccine = vaccines[(vaccines[\"Vaccination status1\"] == 0.0) &\n",
    "                      (vaccines[\"Vaccination status2\"] == 0.0) &\n",
    "                      (vaccines[\"Vaccination status3\"] == 0.0)]\n",
    "any_vaccine = vaccines[(vaccines[\"Vaccination status1\"] == 1) |\n",
    "                       (vaccines[\"Vaccination status2\"] == 1) |\n",
    "                       (vaccines[\"Vaccination status3\"] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our hypothesis tests. As given on page 521 in _Modern Mathematical Statistics with Applications (Devore, Berk)_, the statistic \\\\[ z = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\frac{1}{n} + \\frac{1}{m}\\right)}} \\\\] with \n",
    "\\\\[\\hat{p} = \\frac{X + Y}{n + m}\\\\] and n and m are the sizes of group 1 and group 2, respectivly, is approximately standard normal distributed. Therefore, we can use this statistics to test our hypothesis. \n",
    "\n",
    "Our null hypothesis is \\\\[ H_0: \\mu_1 = \\mu_2\\\\] where the first mu is from a vaccinated group, and the other one is from the unvaccinated. We will reject this hypothesis if the test statistics reaches 2.58 in absolute value, which corresponds to a 99% cofindence interval. The reason we chose this interval is that we have many observations, so if a test statsitic is between 1.96 (corresponding to a 95% interval) and 2.58, the varriance would be pretty high, or the different means would be pretty low. \n",
    "\n",
    "Note that the way the test is set up, a positive z-value means that vaccination groups have more cases of something, while a negative means it has less. In this case, we are therefore mainly interested in negative values, since it means that the vaccines are \"working\", less people get Covid-Positive and die. However, in the next task, 1c), we will look at positive z-values, since it corresponds to an _increased risk of side effects_. \n",
    "\n",
    "Finally, here are some functions for calculating the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaccine_sick_test(df1, df2):\n",
    "    \"\"\"\n",
    "    Hypothesis test on df1 and df2 if you are more probable to get Covid in \n",
    "    group df1 and df2.\n",
    "    \"\"\"\n",
    "    n1 = len(df1) # Number of people in each population\n",
    "    n2 = len(df2)\n",
    "    s1 = len(df1[df1[\"Covid-Positive\"] == 1]) # Number of \"positives\" in each groups\n",
    "    s2 = len(df2[df2[\"Covid-Positive\"] == 1])\n",
    "    p1 = s1/n1 # Ratio of Covid-Positives over whole group\n",
    "    p2 = s2/n2\n",
    "    p_hat = (s1 + s2)/(n1 + n2) # Parameter for test statistic\n",
    "    z_value = (p1 - p2)/np.sqrt((p_hat*(1-p_hat)*(1/n1 + 1/n2)))\n",
    "    return p1, p2, z_value\n",
    "\n",
    "def vaccine_death_test(df1, df2):\n",
    "    \"\"\"\n",
    "    Hypothesis test on if you are more likely to die from covid in df1 than df2.\n",
    "    We only look at people with covid in both groups.\n",
    "    \"\"\"\n",
    "    # n1 = len(df1)\n",
    "    # n2 = len(df2)\n",
    "    n1 = len(df1[df1[\"Covid-Positive\"] == 1]) # Number of people in each population\n",
    "    n2 = len(df2[df2[\"Covid-Positive\"] == 1])\n",
    "    s1 = len(df1[(df1[\"Covid-Positive\"] == 1) & (df1[\"Death\"] == 1)]) \n",
    "    s2 = len(df2[(df2[\"Covid-Positive\"] == 1) & (df2[\"Death\"] == 1)])\n",
    "    p1 = s1/n1 # Ratio of Covid-Deaths over whole group\n",
    "    p2 = s2/n2\n",
    "    p_hat = (s1 + s2)/(n1 + n2) # Parameter for test statistic\n",
    "    z_value = (p1 - p2)/np.sqrt((p_hat*(1-p_hat)*(1/n1 + 1/n2)))\n",
    "    return p1, p2, z_value\n",
    "\n",
    "def get_df_name(df):\n",
    "    \"\"\"\n",
    "    Getting name of a dataframe.\n",
    "    \"\"\"\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing group any_vaccine against non-vaccinated\n",
      "Percentages of people who got covid:\n",
      "    Vaccinated: 19.8556, Unvaccinated: 25.2822\n",
      "    z_value: -20.288224483052876\n",
      "Percentages of Covid-Positive persons that died:\n",
      "    Vaccinated: 0.2352, Unvaccinated: 2.7959\n",
      "    z_value: -16.054053528345055\n",
      "\n",
      "Testing group vaccine1 against non-vaccinated\n",
      "Percentages of people who got covid:\n",
      "    Vaccinated: 22.4264, Unvaccinated: 25.2822\n",
      "    z_value: -7.654631648367728\n",
      "Percentages of Covid-Positive persons that died:\n",
      "    Vaccinated: 0.1354, Unvaccinated: 2.7959\n",
      "    z_value: -10.58646105278172\n",
      "\n",
      "Testing group vaccine2 against non-vaccinated\n",
      "Percentages of people who got covid:\n",
      "    Vaccinated: 20.0781, Unvaccinated: 25.2822\n",
      "    z_value: -14.161658124564054\n",
      "Percentages of Covid-Positive persons that died:\n",
      "    Vaccinated: 0.2492, Unvaccinated: 2.7959\n",
      "    z_value: -9.582089209626343\n",
      "\n",
      "Testing group vaccine3 against non-vaccinated\n",
      "Percentages of people who got covid:\n",
      "    Vaccinated: 17.1234, Unvaccinated: 25.2822\n",
      "    z_value: -22.63006223941343\n",
      "Percentages of Covid-Positive persons that died:\n",
      "    Vaccinated: 0.3466, Unvaccinated: 2.7959\n",
      "    z_value: -8.534733880600648\n"
     ]
    }
   ],
   "source": [
    "groups = [any_vaccine, vaccine1, vaccine2, vaccine3]\n",
    "\n",
    "for group in groups:\n",
    "    p1_s, p2_s, z_s = vaccine_sick_test(group, no_vaccine)\n",
    "    p1_d, p2_d, z_d = vaccine_death_test(group, no_vaccine)\n",
    "    print()\n",
    "    print(f\"Testing group {get_df_name(group)} against non-vaccinated\")\n",
    "    print(f\"Percentages of people who got covid:\")\n",
    "    print(f\"    Vaccinated: {p1_s*100:.4f}, Unvaccinated: {p2_s*100:.4f}\")\n",
    "    print(f\"    z_value: {z_s}\")\n",
    "    print(f\"Percentages of Covid-Positive persons that died:\")\n",
    "    print(f\"    Vaccinated: {p1_d*100:.4f}, Unvaccinated: {p2_d*100:.4f}\")\n",
    "    print(f\"    z_value: {z_d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to interpret the results. Firstly, all of the tests are highly statistic significant. The Covid-Positive statistic is even bigger in absolute value than the deaths, and this is expected since there are much fewer deaths than covid-cases. Howeverthe lowest z-value is 7.65 in absolute value, which is still extremely significant. However, what do the results mean? To answer that, let's compare the ratio in the means for the different groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing group any_vaccine against non-vaccinated\n",
      "    p_u / p_v = 1.2733 for Covid-Positivity, 27.3307% decrease\n",
      "    p_u / p_v = 11.8885 for Covid-Deaths, 1088.8524% decrease\n",
      "Testing group vaccine1 against non-vaccinated\n",
      "    p_u / p_v = 1.1273 for Covid-Positivity, 12.7345% decrease\n",
      "    p_u / p_v = 20.6476 for Covid-Deaths, 1964.7649% decrease\n",
      "Testing group vaccine2 against non-vaccinated\n",
      "    p_u / p_v = 1.2592 for Covid-Positivity, 25.9198% decrease\n",
      "    p_u / p_v = 11.2199 for Covid-Deaths, 1021.9907% decrease\n",
      "Testing group vaccine3 against non-vaccinated\n",
      "    p_u / p_v = 1.4765 for Covid-Positivity, 47.6477% decrease\n",
      "    p_u / p_v = 8.0661 for Covid-Deaths, 706.6143% decrease\n"
     ]
    }
   ],
   "source": [
    "groups = [any_vaccine, vaccine1, vaccine2, vaccine3]\n",
    "\n",
    "for group in groups:\n",
    "    p1_s, p2_s, z_s = vaccine_sick_test(group, no_vaccine)\n",
    "    p1_d, p2_d, z_d = vaccine_death_test(group, no_vaccine)\n",
    "    print(f\"Testing group {get_df_name(group)} against non-vaccinated\")\n",
    "    print(f\"    p_u / p_v = {p2_s/p1_s:.4f} for Covid-Positivity, {(p2_s/p1_s - 1)*100:.4f}% decrease\")\n",
    "    print(f\"    p_u / p_v = {p2_d/p1_d:.4f} for Covid-Deaths, {(p2_d/p1_d - 1)*100:.4f}% decrease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we see that the vaccines are much better at preventing deaths than preventing covid cases. The vaccines makes only 12-48% less people sick from Covid, but reduces Covid related deaths with 7 to 20 times. We also see that the vaccines work in different ways. Vaccine1 have the best decrease in deaths, but the least decrease in sickness. Vaccine3 works the other way around, decreaseing the sickness by almost 50%, but does not decrease the deaths by halves as much as vaccine1. Finally, vaccine2 stands in the middle of these to. \n",
    "\n",
    "From this we conclude that the vaccince efficancy is highly statisticly significant, but have a much bigger impact on covid related deaths than preventing covid in the first place. The vaccines have different strengths and weaknesses. If one were to chose which vaccine to prefer, one would have to assume wether preventing covid or preventing covid related deaths would be the goal, but we are not going to make this decision. \n",
    "\n",
    "Let us consider the weaknesses in this approach. We did not consider all of the data, the genes and comorbidities were overlooked. This was partly because in a) they did not seem very relevant, but also because it would be a lot more work, the possibilities of combinations are pretty high. However, one could assume causality between a cormobidity and a specific vaccine's outcome.Some other methods to consider might be logistic regression and bayesian confidence intervals. \n",
    "Note also that vaccine3 has an artificiall low death increase compared to the other vaccines. Since this vaccine is better at preventing covid in the first place, the total people with covid in the group is lower. This makes the demoninator smaller, and the of people dying from covid higher. One could avoid this problem by also looking at the total death toll in the groups, not just the strictly covid realated ones. One would consider some people to die from something not realted from covid as well, however, it turns out that the non-vaccinated group has no non-covid deaths. Therefore, this consideration becomes a little bit cumbersome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "\n",
    "For this section, we will explore if the vaccines are likely to cause any side effects. We formalize this statement as how likely one are to _not_ be Covid-Positive, but to have one of the other symptoms. We look at the group that is not vaccinated, and hypothesis test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_effect_test(df1, df2, symptom):\n",
    "    \"\"\"\n",
    "    In: \n",
    "        df1: (df) DataFrame of population1\n",
    "        df2: (df) DataFrame of population2\n",
    "        symptom: (str) Column name corresponding to the symptom to be tested.\n",
    "    Out:\n",
    "        p1: (float) Ratio of non-Covid-Positive people that have symptoms in df1\n",
    "        p2: (float) Ratio of non-Covid-Positive people that have symptoms in df1\n",
    "        z_value: (float) z-value according to the hypothesis test (see below). \n",
    "    Tests if there is a significant increase of propability to get the \n",
    "    \"symptom\" as a side effect in df1 than df2, or vice verca. Side effect\n",
    "    is defined as having a symptom, but not being Covid-Positive. \n",
    "    \n",
    "    Hypothesis test (approximated standard normal for binomial data, page 521\n",
    "    in \"Modern Mathematical Statistics with Applications, Devore, Berk\". \n",
    "    z = (p1 - p2)/sqrt(p_hat (1-p_hat) (1/n + 1/m))\n",
    "    where p1 and p2 is ratio between positives and size of sample 1 and 2, \n",
    "    respectivly, n and m are the size of sample 1 and 2, respectivly, and \n",
    "    p_hat is (X + Y)/(n + m), where X and Y are the positives in sample 1 and 2, \n",
    "    once again respectivly. \n",
    "    \"\"\"\n",
    "    sample1 = df1[df1[\"Covid-Positive\"] == 0] \n",
    "    sample2 = df2[df2[\"Covid-Positive\"] == 0]\n",
    "    n1 = len(df1) # Number of people in each population\n",
    "    n2 = len(df2)\n",
    "    s1 = len(sample1[sample1[symptom] == 1]) # Amount of people having the symptom\n",
    "    s2 = len(sample2[sample2[symptom] == 1])\n",
    "    p1 = s1/n1 # Ratio of symptomatic people and non-symptomatic\n",
    "    p2 = s2/n2\n",
    "    p_hat = (s1 + s2)/(n1 + n2) # Parameter for test statistic\n",
    "    z_value = (p1 - p2)/np.sqrt((p_hat*(1-p_hat)*(1/n1 + 1/n2)))\n",
    "    return p1, p2, z_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side effects\n",
    "symptom_names = [\"No-Taste/Smell\", \"Fever\", \"Headache\", \"Pneumonia\", \"Stomach\", \n",
    "                 \"Myocarditis\", \"Blood-Clots\", \"Death\"]\n",
    "for symptom in symptom_names: \n",
    "    p1, p2, z_value = side_effect_test(any_vaccine, no_vaccine, symptom)\n",
    "    print(f\"Symptom {symptom}: Vaccinated {p1*100:.4f}, unvaccinated: {p2*100:.4f}\")\n",
    "    print(f\"    The z-value is {z_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
